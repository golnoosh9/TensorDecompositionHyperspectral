clear all;
%%%according to indian_pines doc bands: 1,33,97,161 were all zeros and not
%%%used

filename= 'indian_pines' ;

load('PaviaU');

load('PaviaU_gt');
image=paviaU;
image_gt=paviaU_gt;
size(image)

ans =

   610   340   103


train_on_image=1;

databaseID=2;%%%1 for salinas, 0 for indian_pines, 2 for pavia U
[r,c,bands]=size(image);
image_added=zeros(r,c,224);


if(databaseID==1)
%      image_added(:, :, 1:108) = image(:, :, 1:108);
%              image_added(:, :, 113:154) = image(:, :, 108:149);
%              image_added(:, :, 168:224) = image(:, :, 149:205);
% 
    image(:,:,[1,33,97,161])=[]; 
end
if(databaseID==2)

    image(:,:,2)=[]; 
end
% else
%     image_added(:,:,1:104)=image(:,:,1:104);
%     image_added(:,:,110:150)=image(:,:,105:145);
%     image_added(:,:,165:220)=image(:,:,145:200);
%     end
% image=image_added;
[r,c,bands]=size(image);
for i=1:bands
    
meanImage=min(min(image(:,:,i)));
image(:,:,i)=image(:,:,i)-meanImage*ones(size(image(:,:,i)));
maxImage=max(max(image(:,:,i)));
image(:,:,i)=image(:,:,i)./maxImage*255;
end


class_num=max(max(image_gt));

T=permute(image,[3 1 2]);;
Ttemp=double(T); 
[bands row_original col_original]=size(Ttemp);

rows=row_original;
cols=col_original;

maxAccuracy=0;
im_predict_max=[];
max_features=[];


flatImage=reshape(image_gt,[rows*cols 1]);

inds=[];
for ii=1:class_num
    ii
    classInds=find(flatImage==ii);
    
   
    size_i=size(classInds,1);
    if(size_i==0)
        continue;
    end
    
 classInds=classInds(randi( size_i,1,floor(0.1*size_i)));
inds=cat(1,inds, classInds);


end

ii =

  uint8

   1


ii =

  uint8

   2


ii =

  uint8

   3


ii =

  uint8

   4


ii =

  uint8

   5


ii =

  uint8

   6


ii =

  uint8

   7


ii =

  uint8

   8


ii =

  uint8

   9


 size_i=size(inds,1);

bsize=2

bsize =

     2

begin=1

begin =

     1

bbegin=1

bbegin =

     1

count=1

count =

     1

bcount=1

bcount =

     1



for bsize=10:2:10
    R=65;
components=65;
trainingNum=65  ;



pcount=1;

if(train_on_image==1)

rand_num=4000;
clear patches;
while(pcount<rand_num)
    
    i=randi([bsize/2,rows-bsize/2-1]);
    j=randi([bsize/2,cols-bsize/2-1]);
    if(image_gt(i,j)>0)
        fcount=1;
       for x=i-bsize/2+1:i+bsize/2
           for y=j-bsize/2+1:j+bsize/2
               patch=Ttemp(:,i,j);
               patches(:,pcount,fcount)=patch;
               fcount=fcount+1;
           end
       end
     
     pcount=pcount+1;      
   end
   
   
end

   

FactorizeTensor;

CreateDictionaryBands;

save('Dictionary','dictionary');

else
   load('Dictionary');
end

create_centered_features_bands;


SVM_train;
ClassAccuracy;


end
Entered factorization
Factor               Size                 Comment                                 
--------------------------------------------------------------------------------
A                    [102x65]             
B                    [3999x65]            
C                    [100x65]             

Factorization        Type       Factors              Comments            
--------------------------------------------------------------------------------
tensor (id = 1)      cpd        A,B,C                                    
reg (id = 2)         regL1      B,A,C                                    
entered dictionary creation
feature calculation enter
Starting parallel pool (parpool) using the 'local' profile ...
connected to 36 workers.

ans = 

 Pool with properties: 

            Connected: true
           NumWorkers: 36
              Cluster: local
        AttachedFiles: {}
    AutoAddClientPath: true
          IdleTimeout: 30 minutes (30 minutes remaining)
          SpmdEnabled: true


  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.5e-06       54-      54  7.2347e+02  2.5229e+01  3.7736e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T -8.6e-06       50-      46  2.4536e+02  2.1251e+01  3.5863e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  3.6e-06       28-      43  1.4189e+03  3.7572e+01  5.2377e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  4.6e-07       65-      59  7.3090e+02  2.6833e+01  1.7326e-03
    1T  5.3e-06       61-      55  2.1730e+02  2.5836e+01  2.1779e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  4.6e-06       61-      59  3.5743e+02  2.9796e+01  3.2999e-03
    1T  1.0e-06       38-      56  1.3041e+03  2.6060e+01  2.3034e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.6e-06       27-      57  7.5510e+02  3.2814e+01  3.8534e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  5.0e-06       65-      36  8.5990e+02  4.3811e+01  9.7846e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T -8.8e-06       33-      53  5.0253e+02  2.7012e+01  1.5914e-03
    1T  3.0e-06       49-      53  4.7444e+02  2.3872e+01  8.6188e-07
    1T  2.2e-06       19-      48  1.8195e+02  2.9378e+01  4.9138e-07
    1T  1.2e-07        2-      54  5.0696e+02  2.3775e+01  8.2875e-09
    1T  3.3e-06       20-      56  5.9138e+02  3.0161e+01  2.2680e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.4e-06       53-      50  4.2956e+02  2.7868e+01  9.6180e-04
    1T  5.0e-06       47-      48  7.2013e+02  2.3771e+01  1.0640e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  1.3e-06       13-      50  2.8303e+02  2.7931e+01  2.4804e-07
    1T -8.6e-07       19-      47  3.9095e+02  2.4061e+01  3.6265e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  7.2e-06       38-      56  2.6816e+02  2.7186e+01  3.2319e-03
    1T  2.6e-06       40-      54  2.9593e+02  2.1935e+01  4.8591e-07

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  7.4e-06       64-      55  4.0650e+02  2.2809e+01  3.2093e-03

indexes =

  Columns 1 through 13

   199    21   153   181    70   120   110   168   171   111    20    83     4

  Columns 14 through 26

   195    55   147    18    45    25    27   166    76   126   131     6   147

  Columns 27 through 30

   139   169   182    66


Accuracy =

   97.7945


Accuracy =

   97.7790


Accuracy =

   97.8023


Accuracy =

   97.7455


Accuracy =

   97.8514


Accuracy =

   97.8255


Accuracy =

   97.8126


Accuracy =

   97.8514


Accuracy =

   97.7790


Accuracy =

   97.8049


Accuracy =9.780487e+01

AverageAccuracy =

   95.3746

clear all;
%%%according to indian_pines doc bands: 1,33,97,161 were all zeros and not
%%%used

filename= 'indian_pines' ;

load('PaviaU');

load('PaviaU_gt');
image=paviaU;
image_gt=paviaU_gt;
size(image)

ans =

   610   340   103


train_on_image=1;

databaseID=2;%%%1 for salinas, 0 for indian_pines, 2 for pavia U
[r,c,bands]=size(image);
image_added=zeros(r,c,224);


if(databaseID==1)
%      image_added(:, :, 1:108) = image(:, :, 1:108);
%              image_added(:, :, 113:154) = image(:, :, 108:149);
%              image_added(:, :, 168:224) = image(:, :, 149:205);
% 
    image(:,:,[1,33,97,161])=[]; 
end
if(databaseID==2)

    image(:,:,2)=[]; 
end
% else
%     image_added(:,:,1:104)=image(:,:,1:104);
%     image_added(:,:,110:150)=image(:,:,105:145);
%     image_added(:,:,165:220)=image(:,:,145:200);
%     end
% image=image_added;
[r,c,bands]=size(image);
for i=1:bands
    
meanImage=min(min(image(:,:,i)));
image(:,:,i)=image(:,:,i)-meanImage*ones(size(image(:,:,i)));
maxImage=max(max(image(:,:,i)));
image(:,:,i)=image(:,:,i)./maxImage*255;
end


class_num=max(max(image_gt));

T=permute(image,[3 1 2]);;
Ttemp=double(T); 
[bands row_original col_original]=size(Ttemp);

rows=row_original;
cols=col_original;

maxAccuracy=0;
im_predict_max=[];
max_features=[];


flatImage=reshape(image_gt,[rows*cols 1]);

inds=[];
for ii=1:class_num
    ii
    classInds=find(flatImage==ii);
    
   
    size_i=size(classInds,1);
    if(size_i==0)
        continue;
    end
    
 classInds=classInds(randi( size_i,1,floor(0.1*size_i)));
inds=cat(1,inds, classInds);


end

ii =

  uint8

   1


ii =

  uint8

   2


ii =

  uint8

   3


ii =

  uint8

   4


ii =

  uint8

   5


ii =

  uint8

   6


ii =

  uint8

   7


ii =

  uint8

   8


ii =

  uint8

   9


 size_i=size(inds,1);

bsize=2

bsize =

     2

begin=1

begin =

     1

bbegin=1

bbegin =

     1

count=1

count =

     1

bcount=1

bcount =

     1



for bsize=10:2:10
    R=65;
components=65;
trainingNum=65  ;



pcount=1;

if(train_on_image==1)

rand_num=4000;
clear patches;
while(pcount<rand_num)
    
    i=randi([bsize/2,rows-bsize/2-1]);
    j=randi([bsize/2,cols-bsize/2-1]);
    if(image_gt(i,j)>0)
        fcount=1;
       for x=i-bsize/2+1:i+bsize/2
           for y=j-bsize/2+1:j+bsize/2
               patch=Ttemp(:,i,j);
               patches(:,pcount,fcount)=patch;
               fcount=fcount+1;
           end
       end
     
     pcount=pcount+1;      
   end
   
   
end

   

FactorizeTensor;

CreateDictionaryBands;

save('Dictionary','dictionary');

else
   load('Dictionary');
end

create_centered_features_bands;


SVM_train;
ClassAccuracy;


end
Entered factorization
Factor               Size                 Comment                                 
--------------------------------------------------------------------------------
A                    [102x65]             
B                    [3999x65]            
C                    [100x65]             

Factorization        Type       Factors              Comments            
--------------------------------------------------------------------------------
tensor (id = 1)      cpd        A,B,C                                    
reg (id = 2)         regL1      B,A,C                                    
entered dictionary creation
feature calculation enter
Starting parallel pool (parpool) using the 'local' profile ...
connected to 36 workers.

ans = 

 Pool with properties: 

            Connected: true
           NumWorkers: 36
              Cluster: local
        AttachedFiles: {}
    AutoAddClientPath: true
          IdleTimeout: 30 minutes (30 minutes remaining)
          SpmdEnabled: true


  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.5e-06       54-      54  7.2347e+02  2.5229e+01  3.7736e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T -8.6e-06       50-      46  2.4536e+02  2.1251e+01  3.5863e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  3.6e-06       28-      43  1.4189e+03  3.7572e+01  5.2377e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  4.6e-07       65-      59  7.3090e+02  2.6833e+01  1.7326e-03
    1T  5.3e-06       61-      55  2.1730e+02  2.5836e+01  2.1779e-03
    1T  1.0e-06       38-      56  1.3041e+03  2.6060e+01  2.3034e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  4.6e-06       61-      59  3.5743e+02  2.9796e+01  3.2999e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.6e-06       27-      57  7.5510e+02  3.2814e+01  3.8534e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  5.0e-06       65-      36  8.5990e+02  4.3811e+01  9.7846e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T -8.8e-06       33-      53  5.0253e+02  2.7012e+01  1.5914e-03
    1T  3.0e-06       49-      53  4.7444e+02  2.3872e+01  8.6188e-07
    1T  1.2e-07        2-      54  5.0696e+02  2.3775e+01  8.2875e-09
    1T  3.3e-06       20-      56  5.9138e+02  3.0161e+01  2.2680e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  2.2e-06       19-      48  1.8195e+02  2.9378e+01  4.9138e-07

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  9.4e-06       53-      50  4.2956e+02  2.7868e+01  9.6180e-04
    1T  1.3e-06       13-      50  2.8303e+02  2.7931e+01  2.4804e-07
    1T  5.0e-06       47-      48  7.2013e+02  2.3771e+01  1.0640e-06

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T -8.6e-07       19-      47  3.9095e+02  2.4061e+01  3.6265e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  2.6e-06       40-      54  2.9593e+02  2.1935e+01  4.8591e-07
    1T  7.2e-06       38-      56  2.6816e+02  2.7186e+01  3.2319e-03

  Itn    xSmall Add/Drop   Active      rNorm2      xNorm1      dyNorm
    1T  7.4e-06       64-      55  4.0650e+02  2.2809e+01  3.2093e-03

indexes =

  Columns 1 through 13

   199    21   153   181    70   120   110   168   171   111    20    83     4

  Columns 14 through 26

   195    55   147    18    45    25    27   166    76   126   131     6   147

  Columns 27 through 30

   139   169   182    66


Accuracy =

   97.7945


Accuracy =

   97.7790


Accuracy =

   97.8023


Accuracy =

   97.7455


Accuracy =

   97.8514


Accuracy =

   97.8255


Accuracy =

   97.8126


Accuracy =

   97.8514


Accuracy =

   97.7790


Accuracy =

   97.8049


Accuracy =9.780487e+01

AverageAccuracy =

   95.3746

